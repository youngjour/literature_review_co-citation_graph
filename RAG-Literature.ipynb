{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Make PDF files as Retrieval-Augmented Generation Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "\n",
    "# pip install pypdf PyPDF2 langchain transformers datasets sentence-transformers langchain-community\n",
    "# pip install google-auth google-auth-oauthlib google-api-python-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\cdsn\\anaconda3\\envs\\yj_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "# from langchain.llms import OpenAI \n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google.oauth2 import service_account\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive API credentials\n",
    "SERVICE_ACCOUNT_FILE = 'path/to/service_account_key.json'  # Update with your file path\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_google_drive():\n",
    "    \"\"\"Authenticate with Google Drive API using a service account.\"\"\"\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES\n",
    "    )\n",
    "    return build('drive', 'v3', credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdfs_from_folder(drive_service, folder_id):\n",
    "    \"\"\"List and download PDF files from a specific Google Drive folder.\"\"\"\n",
    "    query = f\"'{folder_id}' in parents and mimeType='application/pdf'\"\n",
    "    results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    files = results.get('files', [])\n",
    "    \n",
    "    # Download the PDF files\n",
    "    pdf_dir = \"google_drive_pdfs\"\n",
    "    os.makedirs(pdf_dir, exist_ok=True)\n",
    "    \n",
    "    for file in files:\n",
    "        file_id = file['id']\n",
    "        file_name = file['name']\n",
    "        request = drive_service.files().get_media(fileId=file_id)\n",
    "        file_path = os.path.join(pdf_dir, file_name)\n",
    "        \n",
    "        with io.FileIO(file_path, 'wb') as fh:\n",
    "            downloader = MediaIoBaseDownload(fh, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                print(f\"Downloaded {file_name} - {int(status.progress() * 100)}%\")\n",
    "    \n",
    "    return pdf_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work flow\n",
    "1) Document loader\n",
    "2) Text splitter\n",
    "3) Embedding\n",
    "4) Vector Store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparities and access to healthy food in the United States: A review of food\n",
      "deserts literature\n",
      "Renee E. Walkera,b,n, Christopher R. Keanea, Jessica G. Burkea\n",
      "a Department of Behavioral and Community Health Sciences, University of Pittsburgh Graduate School of Public Health, 130 DeSoto Street, Pitt\n"
     ]
    }
   ],
   "source": [
    "# Setting the pdf file location\n",
    "loader = PyPDFLoader('test_pdf/Walker_2010.pdf')\n",
    "\n",
    "# Setting the pdf loader\n",
    "docs = loader.load()\n",
    "\n",
    "# Printing out the loaded document\n",
    "print(docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract text and preprocess it\n",
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(pdf_folder):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            file_path = os.path.join(pdf_folder, file_name)\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_name in os.listdir('test_pdf'):\n",
    "    if file_name.endswith('.pdf'):\n",
    "        file_path = os.path.join('test_pdf', file_name)\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "        documents.extend(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split text into manageable chunks\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate embeddings and store in FAISS\n",
    "def create_vector_store(documents, vectorstore_path, embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metadatas = [doc.metadata for doc in documents]\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    vector_store = FAISS.from_texts(texts, embeddings, metadatas=metadatas)\n",
    "    vector_store.save_local(vectorstore_path)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Load FAISS vector store\n",
    "def load_vector_store(vectorstore_path, embedding_model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    return FAISS.load_local(vectorstore_path, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create a RAG pipeline\n",
    "def create_rag_pipeline(vector_store, llm_model_name=\"decapoda-research/llama-7b-hf\"):\n",
    "    # Load the LLaMA model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(llm_model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "    # Use the LLaMA model in a pipeline\n",
    "    llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "    # Create a retriever and QA chain\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main(pdf_folder, vectorstore_path, llm_model_name=\"decapoda-research/llama-7b-hf\"):\n",
    "    if not os.path.exists(vectorstore_path):\n",
    "        print(\"Extracting and embedding documents...\")\n",
    "        documents = extract_text_from_pdfs(pdf_folder)\n",
    "        split_docs = split_documents(documents)\n",
    "        create_vector_store(split_docs, vectorstore_path)\n",
    "    else:\n",
    "        print(\"Loading existing vector store...\")\n",
    "    \n",
    "    vector_store = load_vector_store(vectorstore_path)\n",
    "    qa_chain = create_rag_pipeline(vector_store, llm_model_name)\n",
    "\n",
    "    # Interactive Q&A\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your query (or 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        result = qa_chain.run(query)\n",
    "        answer = result['answer']\n",
    "        sources = result['source_documents']\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "        print(\"\\nSources:\")\n",
    "        for doc in sources:\n",
    "            print(f\"- {doc.metadata['source']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting and embedding documents...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m pdf_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your folder path\u001b[39;00m\n\u001b[0;32m      3\u001b[0m vectorstore_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_store\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Directory to save FAISS vector store\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(pdf_folder, vectorstore_path, llm_model_name)\u001b[0m\n\u001b[0;32m      5\u001b[0m     documents \u001b[38;5;241m=\u001b[39m extract_text_from_pdfs(pdf_folder)\n\u001b[0;32m      6\u001b[0m     split_docs \u001b[38;5;241m=\u001b[39m split_documents(documents)\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorstore_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading existing vector store...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m, in \u001b[0;36mcreate_vector_store\u001b[1;34m(documents, vectorstore_path, embedding_model_name)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_vector_store\u001b[39m(documents, vectorstore_path, embedding_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m(embedding_model_name)\n\u001b[0;32m      3\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m      4\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    pdf_folder = \"test_pdf\"  # Replace with your folder path\n",
    "    vectorstore_path = \"vector_store\"  # Directory to save FAISS vector store\n",
    "    main(pdf_folder, vectorstore_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
