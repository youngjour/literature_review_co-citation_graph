{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-citation Network Clustering and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import community as community_louvain # python-louvain library\n",
    "from collections import Counter\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# For text processing, if you want more advanced tokenization (optional)\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "# Ensure you have the necessary NLTK data if you use it:\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "GRAPHML_FILE_PATH = \"data/graphml/python_cocitation_network_filtered.graphml\" # Or your specific graphml file\n",
    "NUMBER_OF_TOP_CLUSTERS_TO_SHOW = 10\n",
    "NUMBER_OF_TOP_PAPERS_PER_CLUSTER = 10\n",
    "NUMBER_OF_TOP_KEYWORDS_PER_CLUSTER = 15\n",
    "MIN_CLUSTER_SIZE_FOR_ANALYSIS = 5 # Minimum number of nodes for a cluster to be analyzed\n",
    "\n",
    "# Define stopwords (customize as needed)\n",
    "STOPWORDS = set([\n",
    "    \"the\", \"a\", \"an\", \"is\", \"are\", \"was\", \"were\", \"of\", \"and\", \"to\", \"in\", \"it\", \"that\", \"this\",\n",
    "    \"for\", \"on\", \"with\", \"as\", \"by\", \"at\", \"from\", \"about\", \"into\", \"onto\", \"through\", \"over\",\n",
    "    \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "    \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\",\n",
    "    \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\",\n",
    "    \"don\", \"should\", \"now\", \"d\", \"ll\", \"m\", \"o\", \"re\", \"ve\", \"y\", \"ain\", \"aren\", \"couldn\",\n",
    "    \"didn\", \"doesn\", \"hadn\", \"hasn\", \"haven\", \"isn\", \"ma\", \"mightn\", \"mustn\", \"needn\", \"shan\",\n",
    "    \"shouldn\", \"wasn\", \"weren\", \"won\", \"wouldn\", \"fig\", \"figure\", \"table\", \"abstract\", \"introduction\",\n",
    "    \"results\", \"discussion\", \"conclusion\", \"references\", \"et\", \"al\", \"paper\", \"study\", \"method\",\n",
    "    \"analysis\", \"based\", \"using\", \"system\", \"approach\", \"model\", \"data\", \"research\",\n",
    "    \"urban\", \"computing\", \"city\", \"cities\", \"science\", \"review\", \"survey\", \"p\", \"pp\", \"vol\", \"v\",\n",
    "    \"issue\", \"journal\", \"conference\", \"workshop\", \"chapter\", \"university\", \"department\", \"dept\",\n",
    "    \"institute\", \"ieee\", \"acm\", \"elsevier\", \"springer\", \"wiley\", \"press\", \"proc\", \"int\", \"j\", \"ann\",\n",
    "    \"soc\", \"sci\", \"technol\", \"res\", \"commun\", \"comput\", \"syst\", \"eur\", \"lect\", \"notes\", \"adv\", \n",
    "    \"ser\", \"trans\", \"eng\", \"manag\", \"appl\", \"rev\", \"lett\", \"rep\", \"bull\", \"mem\", \"assoc\", \"symp\",\n",
    "    \"inc\", \"ltd\", \"corp\", \"org\", \"co\", \"ed\", \"eds\", \"vol\", \"no\", \"pp\", \"chap\", \"art\", \"isbn\",\n",
    "    \"doi\", \"http\", \"https\", \"www\", \"com\", \"org\", \"net\", \"pdf\", \"html\", \"gov\", \"edu\",\n",
    "    # Common author initials/short names (can be expanded)\n",
    "    \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\",\n",
    "    \"dr\", \"prof\", \"phd\", \"ms\", \"mr\", \"mrs\", \"inc\", \"corp\", \"ltd\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Co-citation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_path = Path(GRAPHML_FILE_PATH)\n",
    "if not graph_path.exists():\n",
    "    print(f\"ERROR: GraphML file not found at {graph_path}\")\n",
    "    print(\"Please ensure 'build_network.py' has been run and the file path is correct.\")\n",
    "    G = None\n",
    "else:\n",
    "    G = nx.read_graphml(graph_path)\n",
    "    print(f\"Graph loaded from {graph_path}\")\n",
    "    print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "\n",
    "# Ensure node attributes are of correct type (freq should be int, year str/int)\n",
    "if G is not None:\n",
    "    for node, data in G.nodes(data=True):\n",
    "        if 'freq' in data:\n",
    "            try:\n",
    "                data['freq'] = int(data['freq'])\n",
    "            except (ValueError, TypeError):\n",
    "                data['freq'] = 0 # Default if conversion fails\n",
    "        else:\n",
    "            data['freq'] = 0\n",
    "        \n",
    "        if 'year' in data:\n",
    "            data['year'] = str(data['year']) # Keep as string for now, convert to int when needed\n",
    "        else:\n",
    "            data['year'] = 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graph Preprocessing (Largest Connected Component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G is not None and G.number_of_nodes() > 0:\n",
    "    # Convert to undirected graph for Louvain if it's directed\n",
    "    if G.is_directed():\n",
    "        G_undirected = G.to_undirected()\n",
    "        print(\"Converted graph to undirected for community detection.\")\n",
    "    else:\n",
    "        G_undirected = G\n",
    "    \n",
    "    # Find the largest connected component (LCC)\n",
    "    # Community detection algorithms often work best on connected graphs.\n",
    "    connected_components = list(nx.connected_components(G_undirected))\n",
    "    if connected_components:\n",
    "        largest_component_nodes = max(connected_components, key=len)\n",
    "        G_lcc = G_undirected.subgraph(largest_component_nodes).copy()\n",
    "        print(f\"Largest Connected Component (LCC) selected for analysis:\")\n",
    "        print(f\"  Nodes in LCC: {G_lcc.number_of_nodes()}\")\n",
    "        print(f\"  Edges in LCC: {G_lcc.number_of_edges()}\")\n",
    "    else:\n",
    "        print(\"Graph has no connected components (it might be empty or all isolated nodes).\")\n",
    "        G_lcc = G_undirected # Fallback to original graph if no components\n",
    "else:\n",
    "    G_lcc = None\n",
    "    print(\"Graph is not loaded or is empty. Skipping further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Community Detection (Louvain Algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G_lcc is not None and G_lcc.number_of_nodes() > 0:\n",
    "    print(\"\\nPerforming community detection using Louvain algorithm...\")\n",
    "    # Compute the best partition using Louvain algorithm, considering edge weights\n",
    "    partition = community_louvain.best_partition(G_lcc, weight='weight', random_state=42)\n",
    "    \n",
    "    # Add community information to node attributes in G_lcc\n",
    "    nx.set_node_attributes(G_lcc, partition, 'community')\n",
    "    \n",
    "    num_communities = len(set(partition.values()))\n",
    "    print(f\"Number of communities found: {num_communities}\")\n",
    "    \n",
    "    if num_communities > 0:\n",
    "        modularity = community_louvain.modularity(partition, G_lcc, weight='weight')\n",
    "        print(f\"Modularity of the partition: {modularity:.4f}\")\n",
    "    else:\n",
    "        print(\"No communities were found.\")\n",
    "else:\n",
    "    partition = {}\n",
    "    num_communities = 0\n",
    "    print(\"Skipping community detection as graph is not suitable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Helper Functions for Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(text):\n",
    "    \"\"\"A very basic tokenizer that splits by non-alphanumeric and converts to lower.\n",
    "       It also tries to handle hyphenated words and common journal patterns.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Preserve hyphens within words, remove leading/trailing hyphens after split\n",
    "    text = re.sub(r'([a-zA-Z])-|-(?=[a-zA-Z])', r'\\1', text.lower())\n",
    "    words = re.split(r'[^a-z0-9-]+', text)\n",
    "    return [word.strip('-') for word in words if word.strip('-') and len(word.strip('-')) > 1] # Min length 2\n",
    "\n",
    "def extract_keywords_from_node_attributes(nodes_data_list, stopwords_set, top_n=10):\n",
    "    \"\"\"Extracts keywords from 'source' and 'author' attributes of nodes in a cluster.\"\"\"\n",
    "    all_words = []\n",
    "    for node_data in nodes_data_list:\n",
    "        # Extract text from source (journal/conference name)\n",
    "        source_text = node_data.get('source', '')\n",
    "        if isinstance(source_text, str):\n",
    "            words = simple_tokenizer(source_text)\n",
    "            all_words.extend([w for w in words if w not in stopwords_set and not w.isdigit()])\n",
    "        \n",
    "        # Optionally, extract from author names (might be less topical)\n",
    "        # author_text = node_data.get('author', '')\n",
    "        # if isinstance(author_text, str):\n",
    "        #     words = simple_tokenizer(author_text)\n",
    "        #     all_words.extend([w for w in words if w not in stopwords_set and not w.isdigit()])\n",
    "            \n",
    "    if not all_words:\n",
    "        return []\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    return [word for word, count in word_counts.most_common(top_n)]\n",
    "\n",
    "def get_cluster_details(graph, partition_map, min_size_for_analysis, num_top_papers, num_top_keywords, stopwords_set):\n",
    "    \"\"\"Analyzes each cluster to find top papers and keywords.\"\"\"\n",
    "    cluster_summaries = []\n",
    "    if not partition_map:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Group nodes by community ID\n",
    "    communities = defaultdict(list)\n",
    "    for node, comm_id in partition_map.items():\n",
    "        communities[comm_id].append(node)\n",
    "\n",
    "    # Sort communities by size (number of nodes)\n",
    "    sorted_communities = sorted(communities.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "\n",
    "    print(f\"\\nAnalyzing top communities (min size: {min_size_for_analysis})...\")\n",
    "    for comm_id, nodes_in_comm in sorted_communities:\n",
    "        if len(nodes_in_comm) < min_size_for_analysis:\n",
    "            continue\n",
    "        \n",
    "        # Get data for nodes in this community\n",
    "        nodes_data_in_comm = [graph.nodes[node] for node in nodes_in_comm if node in graph.nodes]\n",
    "        \n",
    "        # Sort nodes by 'freq' (overall citation count of the cited reference) descending\n",
    "        # The 'label' attribute is the \"AUTHOR, YEAR, SOURCE\" string\n",
    "        sorted_nodes_by_freq = sorted(nodes_data_in_comm, key=lambda x: x.get('freq', 0), reverse=True)\n",
    "        \n",
    "        top_papers_info = []\n",
    "        for node_data in sorted_nodes_by_freq[:num_top_papers]:\n",
    "            label = node_data.get('label', 'Unknown Label')\n",
    "            freq = node_data.get('freq', 0)\n",
    "            year = node_data.get('year', 'Unknown')\n",
    "            top_papers_info.append(f\"{label} (Freq: {freq}, Year: {year})\")\n",
    "            \n",
    "        # Extract keywords from all nodes in the community\n",
    "        keywords = extract_keywords_from_node_attributes(nodes_data_in_comm, stopwords_set, num_top_keywords)\n",
    "        \n",
    "        # Calculate average year and year range for the cluster (cited reference years)\n",
    "        years_in_comm = [int(data['year']) for data in nodes_data_in_comm if data['year'].isdigit()]\n",
    "        avg_year = np.mean(years_in_comm) if years_in_comm else 'N/A'\n",
    "        min_year = min(years_in_comm) if years_in_comm else 'N/A'\n",
    "        max_year = max(years_in_comm) if years_in_comm else 'N/A'\n",
    "        year_range_str = f\"{min_year}-{max_year}\" if min_year != 'N/A' else 'N/A'\n",
    "        \n",
    "        cluster_summaries.append({\n",
    "            'Cluster ID': comm_id,\n",
    "            'Size (Nodes)': len(nodes_in_comm),\n",
    "            'Avg. Cited Year': f\"{avg_year:.1f}\" if isinstance(avg_year, float) else avg_year,\n",
    "            'Cited Year Range': year_range_str,\n",
    "            f'Top {num_top_papers} Cited Refs': \"\\n\".join(top_papers_info),\n",
    "            f'Top {num_top_keywords} Keywords': \", \".join(keywords)\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(cluster_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Analyze and Display Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G_lcc is not None and num_communities > 0:\n",
    "    cluster_summary_df = get_cluster_details(\n",
    "        G_lcc, \n",
    "        partition, \n",
    "        MIN_CLUSTER_SIZE_FOR_ANALYSIS, \n",
    "        NUMBER_OF_TOP_PAPERS_PER_CLUSTER, \n",
    "        NUMBER_OF_TOP_KEYWORDS_PER_CLUSTER, \n",
    "        STOPWORDS\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n--- Summary of Top {min(NUMBER_OF_TOP_CLUSTERS_TO_SHOW, len(cluster_summary_df))} Largest Communities ---\")\n",
    "    if not cluster_summary_df.empty:\n",
    "        # To display long text fields better in pandas\n",
    "        pd.set_option('display.max_colwidth', 200)\n",
    "        display(cluster_summary_df.head(NUMBER_OF_TOP_CLUSTERS_TO_SHOW))\n",
    "    else:\n",
    "        print(\"No clusters met the minimum size for analysis.\")\n",
    "else:\n",
    "    print(\"\\nNo communities to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Temporal Analysis of Nodes within Static Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G_lcc is not None and not cluster_summary_df.empty:\n",
    "    print(\"\\n--- Temporal Analysis of Cited References within Top Clusters ---\")\n",
    "    \n",
    "    # Analyze a few top clusters for brevity\n",
    "    num_clusters_for_temporal_plot = min(5, len(cluster_summary_df))\n",
    "    \n",
    "    if num_clusters_for_temporal_plot > 0:\n",
    "        fig, axes = plt.subplots(num_clusters_for_temporal_plot, 1, figsize=(12, 4 * num_clusters_for_temporal_plot), sharex=True)\n",
    "        if num_clusters_for_temporal_plot == 1:\n",
    "            axes = [axes] # Make it iterable if only one subplot\n",
    "            \n",
    "        all_node_years = [int(data['year']) for node, data in G_lcc.nodes(data=True) if data['year'].isdigit()]\n",
    "        if not all_node_years:\n",
    "            print(\"No valid year data found in graph nodes for temporal analysis.\")\n",
    "        else:\n",
    "            min_graph_year = min(all_node_years) if all_node_years else 1900\n",
    "            max_graph_year = max(all_node_years) if all_node_years else 2025\n",
    "            year_bins = np.arange(min_graph_year, max_graph_year + 2) # +2 for right edge of last bin\n",
    "\n",
    "            for i in range(num_clusters_for_temporal_plot):\n",
    "                cluster_id = cluster_summary_df.loc[i, 'Cluster ID']\n",
    "                nodes_in_comm = [node for node, comm in partition.items() if comm == cluster_id]\n",
    "                \n",
    "                node_years_in_comm = []\n",
    "                for node_id in nodes_in_comm:\n",
    "                    year_str = G_lcc.nodes[node_id].get('year', 'Unknown')\n",
    "                    if year_str.isdigit():\n",
    "                        node_years_in_comm.append(int(year_str))\n",
    "                \n",
    "                if node_years_in_comm:\n",
    "                    ax = axes[i]\n",
    "                    ax.hist(node_years_in_comm, bins=year_bins, edgecolor='black', alpha=0.7)\n",
    "                    ax.set_title(f\"Cluster {cluster_id} (Size: {len(nodes_in_comm)}) - Cited Reference Year Distribution\")\n",
    "                    ax.set_ylabel(\"Number of Cited Refs\")\n",
    "                    avg_year_comm = np.mean(node_years_in_comm)\n",
    "                    ax.axvline(avg_year_comm, color='red', linestyle='dashed', linewidth=1, label=f'Avg Year: {avg_year_comm:.1f}')\n",
    "                    ax.legend()\n",
    "                else:\n",
    "                    axes[i].set_title(f\"Cluster {cluster_id} - No valid year data for nodes\")\n",
    "            \n",
    "            axes[-1].set_xlabel(\"Publication Year of Cited Reference\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No clusters large enough for temporal plot analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualization of Clustered Graph (LCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if G_lcc is not None and G_lcc.number_of_nodes() > 0 and num_communities > 0:\n",
    "    print(\"\\nVisualizing the largest connected component with communities...\")\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    # Use spring_layout or kamada_kawai_layout. Kamada-Kawai can be slow for large graphs.\n",
    "    # For very large graphs, consider a simpler layout or visualizing subgraphs.\n",
    "    if G_lcc.number_of_nodes() < 500: # Kamada-Kawai for smaller graphs\n",
    "        pos = nx.kamada_kawai_layout(G_lcc, weight='weight')\n",
    "    else: # Spring layout for larger graphs (can be adjusted)\n",
    "        pos = nx.spring_layout(G_lcc, k=0.1, iterations=50, weight='weight', seed=42)\n",
    "        \n",
    "    # Node colors based on community\n",
    "    community_ids = [partition[node] for node in G_lcc.nodes()]\n",
    "    cmap = cm.get_cmap('viridis', max(community_ids) + 1) # Or 'tab20', 'Set3'\n",
    "    \n",
    "    # Node sizes based on frequency ('freq' attribute)\n",
    "    node_sizes = [G_lcc.nodes[node].get('freq', 1) * 20 + 10 for node in G_lcc.nodes()] # Scale for visibility\n",
    "    \n",
    "    nx.draw_networkx_nodes(G_lcc, pos, node_color=community_ids, cmap=cmap, node_size=node_sizes, alpha=0.8)\n",
    "    nx.draw_networkx_edges(G_lcc, pos, alpha=0.2, width=0.5)\n",
    "    \n",
    "    # Optional: Add labels to very important nodes (e.g., top N by frequency)\n",
    "    # sorted_nodes_for_labels = sorted(G_lcc.nodes(data=True), key=lambda x: x[1].get('freq', 0), reverse=True)\n",
    "    # labels_to_draw = {node: data.get('label', '')[:20]+\"...\" for node, data in sorted_nodes_for_labels[:10]} # Label top 10\n",
    "    # nx.draw_networkx_labels(G_lcc, pos, labels=labels_to_draw, font_size=8)\n",
    "    \n",
    "    plt.title(f\"Co-citation Network (LCC) - Colored by Louvain Community (k={num_communities}, Modularity={modularity:.3f})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping graph visualization as graph is not suitable or no communities found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Discussion on Time-Series Co-citation Analysis (Dynamic Communities)\n",
    "\n",
    "Your coworker's notebook (`citation (1).ipynb`) includes a section for \"Time-series Co-citation Network Analysis.\" This type of analysis typically involves creating graph snapshots for different time periods (e.g., yearly) and then detecting communities within each snapshot to see how topics evolve.\n",
    "\n",
    "**Key Requirement for Dynamic Community Analysis:**\n",
    "To perform this, the **edges** in your co-citation graph need a 'year' attribute, representing the publication year of the *citing paper* that created the co-citation link. Your current `build_network.py` script (as last revised) adds a 'year' attribute to *nodes* (the publication year of the cited reference itself), but not to edges.\n",
    "\n",
    "**How to Adapt for Dynamic Analysis (if desired):**\n",
    "\n",
    "1.  **Modify `build_network.py`:**\n",
    "    * When parsing WoS files in `parse_wos_file`, ensure you capture the Publication Year (`PY`) of the *citing paper*.\n",
    "    * In `build_cocitation_network`, when you create co-citation edges (`cocitation_links[edge] += 1`), you would also need to store the `PY` of the *citing paper* that generated this specific co-citation instance. If an edge (a pair of co-cited references) is formed by multiple citing papers from different years, you might store a list of years or the earliest/latest year.\n",
    "    * When saving to GraphML in `save_graph_to_graphml`, ensure this 'year' (or 'years') attribute is written for each edge.\n",
    "\n",
    "2.  **Adapt Analysis Notebook:**\n",
    "    * Once your GraphML file contains 'year' attributes on edges, you can create yearly subgraphs:\n",
    "        ```python\n",
    "        # Example:\n",
    "        # G_with_edge_years = nx.read_graphml(\"path_to_your_new_graphml_with_edge_years.graphml\")\n",
    "        # unique_edge_years = sorted(list(set(d['year'] for u,v,d in G_with_edge_years.edges(data=True) if 'year' in d)))\n",
    "        # subgraphs_by_year = {}\n",
    "        # for year in unique_edge_years:\n",
    "        #     edges_this_year = [(u,v) for u,v,d in G_with_edge_years.edges(data=True) if d.get('year') == year]\n",
    "        #     subG = G_with_edge_years.edge_subgraph(edges_this_year).copy()\n",
    "        #     # Remove isolated nodes from the subgraph for cleaner community detection\n",
    "        #     subG.remove_nodes_from(list(nx.isolates(subG)))\n",
    "        #     subgraphs_by_year[year] = subG\n",
    "        ```\n",
    "    * Then, you can run community detection on each `subG` in `subgraphs_by_year` and analyze the evolution of communities, similar to cells 16-19 in your coworker's notebook (e.g., tracking community size, top papers, Jaccard similarity between communities in consecutive years).\n",
    "\n",
    "**Alternative Temporal Insight (with current graph):**\n",
    "The \"Temporal Analysis of Nodes within Static Clusters\" section (Section 6) already provides some temporal insights by looking at the distribution of the publication years of the *cited references* within the clusters found in the overall static graph. This can indicate whether a cluster focuses on foundational (older) literature or more recent developments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
